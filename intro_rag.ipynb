{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDsTUvKjwHBW"
   },
   "source": [
    "# Multimodal Retrieval Augmented Generation (RAG) using Vertex AI Gemini API\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/Analystics-AI/Guideline_RAG/tree/main/intro_rag.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | \n",
    "|-|-|\n",
    "|Author(s) | [Manu Weissel](https://www.linkedin.com/in/man%C3%BA-weissel-618127211/) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "Retrieval-Augmented Generation (RAG) is a groundbreaking technique in the field of natural language processing (NLP) that merges the extensive knowledge capacity of large language models (LLMs) with the specificity and relevance of information retrieval systems. By uploading relevant documents, RAG models can generate responses that are not only contextually accurate but also deeply informed by relevant external data sources. \n",
    "Retrieval augmented generation (RAG) has become a popular paradigm for enabling LLMs to access external data and also as a mechanism for grounding to mitigate against hallucinations.\n",
    "\n",
    "The RAG approach involves two main steps:\n",
    "\n",
    "1. **Retrieval of Information:** The model searches a large dataset or corpus to find pieces of information that are relevant to the input query. This dataset can be anything from the entirety of Wikipedia to a specialized database tailored to a specific domain.\n",
    "2. **Generation of Responses:** The model then combines this retrieved information with the original query to generate a response. This process leverages the power of generative models, like GPT (Generative Pre-trained Transformer), BERT (Bidirectional and Auto-Regressive Transformers) or Gemini, which are trained to produce coherent and contextually relevant text based on the inputs they receive.\n",
    "\n",
    "### Gemini\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision and Gemini 1.0 Pro models.\n",
    "\n",
    "### Comparing text-based and multimodal RAG\n",
    "Multimodal RAG offers several advantages over text-based RAG:\n",
    "\n",
    "1. **Enhanced knowledge access:** Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
    "2. **Improved reasoning capabilities:** By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
    "\n",
    "This notebook shows you how to use RAG with Vertex AI Gemini API, [text embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings), and [multimodal embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/multimodal-embeddings), to build a document search engine.\n",
    "\n",
    "Through hands-on examples, you will discover how to construct a multimedia-rich metadata repository of your document sources, enabling search, comparison, and reasoning across diverse information streams. Summarised you will learn how to perform multimodal RAG where you can perform Q&A over a financial document filled with both text and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnpYxfesh2rI"
   },
   "source": [
    "# Set up Vertex AI\n",
    "\n",
    "\n",
    "This tutorial uses components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "In order to access Vertex AI and create a free-trial account, following steps are necessary:.\n",
    "1. Enter your Google credentials [here](https://console.cloud.google.com/freetrial?hl=de&facet_utm_source=%28direct%29&facet_utm_campaign=%28direct%29&facet_utm_medium=%28none%29&facet_url=https%3A%2F%2Fcloud.google.com%2Fvertex-ai%2Fpricing&facet_id_list=%5B39300012%2C+39300022%2C+39300118%2C+39300195%2C+39300251%2C+39300317%2C+39300320%2C+39300326%2C+39300345%2C+39300354%2C+39300364%2C+39300373%2C+39300412%2C+39300421%2C+39300436%2C+39300471%2C+39300488%2C+39300496%2C+39300498%5D&_ga=2.268870644.400606400.1709743568-1134076064.1709743561). If you don't have a google Account you need to create one.\n",
    "2. You will have to enter your credit card information, but there will be NO costs.\n",
    "3. Follow the required steps to use the Google Cloud Platform services. \n",
    "4. You will be able to use Vertex AI for the next 90 days.\n",
    "5. Click on following Icon to open this code in you google vertex AI workbench.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/Analystics-AI/Guideline_RAG/main/intro_rag.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. On the right side of the page you can create a user managed notebook instance. Follow the mentioned steps.\n",
    "- Please start the tutorial and then stay at the first step. Since your project 'My First Project' was already created, you can proceed to the second part of the first step. Don't upgrade your Free Trail if you want to avoid uncessary costs. \n",
    "- Press 'Enable the Notebooks API'. This can take up to 5 minutes. DON'T rush to the next step. \n",
    "- After enableing the Notebooks API the layout should change and after scrolling down you should be able to press create. Do not proceed with any other steps. \n",
    "7. After proceeding with these steps a notebook with JupyterLab should open itself. After loading click on 'OPEN', confirm the deployment and then you are ready to go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please proceed if and ONLY if you have set up Vertex AI in JupyterLab on Google Cloud Workspace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "\n",
    "### Objectives\n",
    "\n",
    "This notebook provides a guide to building a document search engine using multimodal retrieval augmented generation (RAG), step by step:\n",
    "\n",
    "1. Extract and store metadata of documents containing both text and images.\n",
    "2. Search the metadata with text queries to find similar text or images\n",
    "3. Search the metadata with image queries to find similar images\n",
    "4. Using a text query as input, search for contexual answers using both text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXJpXzKrh2rJ"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "executionInfo": {
     "elapsed": 23606,
     "status": "ok",
     "timestamp": 1707899661283,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "kc4WxYmLSBW5",
    "outputId": "41191d43-b2b3-4bfd-c5e5-c0a53f03d1e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform pymupdf\n",
    "! pip install gdown\n",
    "! pip install Gensim\n",
    "! pip install spacy\n",
    "! pip install nltk\n",
    "! python -m spacy download en_core_web_lg \n",
    "! pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel. You only need to do this the first time you access the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1707913872852,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "XRvKdaPDTznN",
    "outputId": "759357fb-7531-4423-a75f-b896af19ce37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The notebook kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1vKZZoEh2rL"
   },
   "source": [
    "### Define Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1707913890887,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "gJqZ76rJh2rM",
    "outputId": "ed164f41-97f3-411d-8cba-d853d71957b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D48gUW5-h2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1707913944466,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "rtMowvm-yQ97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-TX_R_xh2rM"
   },
   "source": [
    "### Load the Gemini 1.0 Pro and Gemini 1.0 Pro Vision model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707913901768,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "SvMwSRJJh2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lCfREXK5SWD"
   },
   "source": [
    "### Download custom Python modules and utilities\n",
    "\n",
    "The cell below will download some helper functions needed for this notebook, to improve readability. You can also view the code (`intro_multimodal_rag_utils.py`) directly on [Github](https://raw.githubusercontent.com/manuyweissel/Master_Thesis_RAG_Guideline/main/utils/intro_multimodal_rag_utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhiFT91850ZZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "if not os.path.exists(\"utils\"):\n",
    "    os.makedirs(\"utils\")\n",
    "\n",
    "\n",
    "# download the helper scripts from utils folder\n",
    "url_prefix = \"https://raw.githubusercontent.com/Analystics-AI/Guideline_RAG/main/utils/\"\n",
    "files = [\"intro_multimodal_rag_utils.py\"]\n",
    "\n",
    "for fname in files:\n",
    "    urllib.request.urlretrieve(f\"{url_prefix}/{fname}\", filename=f\"utils/{fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7bKCQMFT7JT"
   },
   "source": [
    "#### Get documents and images from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1707913903524,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "KwbL89zcY39N",
    "outputId": "baa3a478-4d55-4b9c-e02f-6816a1d589b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download documents and images used in this notebook\n",
    "import gdown\n",
    "!gdown --folder https://drive.google.com/drive/folders/1jy-bwju_T6aOA4Ym69oswhbmD5HNHo48\n",
    "print(\"Download completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ If you use your own data exchange the files in the folder \"data\". ⚠️</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps1G-cCfpibN"
   },
   "source": [
    "## Building metadata of documents containing text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqLsy3iZ5t-R"
   },
   "source": [
    "### The data\n",
    "\n",
    "The source data that you will use in this notebook is the [BMW Earning Call Transcript](https://drive.google.com/file/d/1ntmUsw_k2kOAsLjMoC8twZYo6rM_yHn_/view?usp=sharing) from the first two quarters in 2023, which provides a comprehensive overview of the company's financial performance, business operations, management, and risk factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvt0sus5KSNX"
   },
   "source": [
    "### Import helper functions to build metadata\n",
    "\n",
    "Before building the multimodal RAG system, it's important to have metadata of all the text and images in the document. For references and citations purposes, the metadata should contain essential elements, including page number, file name, image counter, and so on. Hence, as a next step, you will generate embeddings from the metadata, which will is required to perform similarity search when quering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.intro_multimodal_rag_utils import get_document_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOAkYN0KlSL"
   },
   "source": [
    "### Extract and store metadata of text and images from a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9hBPPWs5CMd"
   },
   "source": [
    "You just imported a function called `get_document_metadata()`. This function extracts text and image metadata from a document, and returns two dataframes, namely *text_metadata* and *image_metadata*, as outputs. If you want to find out more about how `get_document_metadata()` function is implemented using Gemini and the embedding models, you can take look at the [source code](https://raw.githubusercontent.com/manuyweissel/Master_Thesis_RAG_Guideline/main/utils/intro_multimodal_rag_utils.py) directly.\n",
    "\n",
    "The reason for extraction and storing both text metadata and image metadata is that just by using either of the two alone is not sufficient to come out with a relevent answer. For example, the relevant answers could be in visual form within a document, but text-based RAG won't be able to take into consideration of the visual images. You will also be exploring this example later in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnKru0sBh2rN"
   },
   "source": [
    "At the next step, you will use the function to extract and store metadata of text and images froma document. Please note that the following cell may take a few minutes to complete:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "The current implementation works best: \n",
    "\n",
    "1) if your documents are a combination of text and images. \n",
    "2) if the tables in your documents are available as images. \n",
    "3) if the images in the document don't require too much context. \n",
    "\n",
    "Additionally, \n",
    "\n",
    "1) If you want to run this on text-only documents, use normal RAG\n",
    "2) If your documents contain particular domain knowledge, pass that information in the prompt below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87150,
     "status": "ok",
     "timestamp": 1707914065279,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "X8hE0tWD-lf8",
    "outputId": "e7a363ca-34a0-49cb-8d95-4d60b5f002ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the PDF folder with multiple PDF\n",
    "\n",
    "# pdf_folder_path = \"/content/data/\" # if running in Google Colab/Colab Enterprise\n",
    "pdf_folder_path = \"data/\"  # if running in Vertex AI Workbench.\n",
    "\n",
    "# Specify the image description prompt. Change it\n",
    "image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "If it's a table, extract all elements of the table.\n",
    "If it's a graph, explain the findings in the graph.\n",
    "Do not include any numbers that are not mentioned in the image.\n",
    "\"\"\"\n",
    "\n",
    "# Extract text and image metadata from the PDF document\n",
    "text_metadata_df, image_metadata_df = get_document_metadata(\n",
    "    multimodal_model,  # we are passing gemini 1.0 pro vision model\n",
    "    pdf_folder_path,\n",
    "    image_save_dir=\"images\",\n",
    "    image_description_prompt=image_description_prompt,\n",
    "    embedding_size=1408,\n",
    "    # add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
    "    # sleep_time_after_page = 5,\n",
    "    # generation_config = # see next cell\n",
    "    # safety_settings =  # see next cell\n",
    ")\n",
    "\n",
    "print(\"\\n\\n --- Completed processing. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Parameters for Gemini API call.\n",
    "# # reference for parameters: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n",
    "\n",
    "# generation_config=  GenerationConfig(temperature=0.2, max_output_tokens=2048)\n",
    "\n",
    "# # Set the safety settings if Gemini is blocking your content or you are facing \"ValueError(\"Content has no parts\")\" error or \"Exception occured\" in your data.\n",
    "# # ref for settings and thresholds: https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes\n",
    "\n",
    "# safety_settings = {\n",
    "#                   HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   }\n",
    "\n",
    "# # You can also pass parameters and safety_setting to \"get_gemini_response\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBBoEXwh2rN"
   },
   "source": [
    "#### Inspect the processed text metadata\n",
    "\n",
    "\n",
    "The follow cell will produce a metadata table which describes the different parts of text metadata, including:\n",
    "\n",
    "- **text**: the original text from the page\n",
    "- **text_embedding_page**: the embedding of the original text from the page\n",
    "- **chunk_text**: the original text divided into smaller chunks\n",
    "- **chunk_number**: the index of each text chunk\n",
    "- **text_embedding_chunk**: the embedding of each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1707914161561,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "6t3AIGFar8Mo",
    "outputId": "19a1a1dd-ea67-4633-9566-468414573962",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjIQYI3mh2rO"
   },
   "source": [
    "#### Inspect the processed image metadata\n",
    "\n",
    "The following cell will produce a metadata table which describes the different parts of image metadata, including:\n",
    "* **img_desc**: Gemini-generated textual description of the image.\n",
    "* **mm_embedding_from_text_desc_and_img**: Combined embedding of image and its description, capturing both visual and textual information.\n",
    "* **mm_embedding_from_img_only**: Image embedding without description, for comparison with description-based analysis.\n",
    "* **text_embedding_from_image_description**: Separate text embedding of the generated description, enabling textual analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1707914169688,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "tkHtAYIK-y-q",
    "outputId": "cfbe50c1-dcd5-4f91-902f-88880e35cc7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBhoOkutUtPr"
   },
   "source": [
    "### Import the helper functions to implement RAG\n",
    "\n",
    "You will be importing the following functions which will be used in the remainder of this notebook to implement RAG:\n",
    "\n",
    "* **get_similar_text_from_query():** Given a text query, finds text from the document which are relevant, using cosine similarity algorithm. It uses text embeddings from the metadata to compute and the results can be filtered by top score, page/chunk number, or embedding size.\n",
    "* **print_text_to_text_citation():** Prints the source (citation) and details of the retrieved text from the `get_similar_text_from_query()` function.\n",
    "* **get_similar_image_from_query():** Given an image path or an image, finds images from the document which are relevant. It uses image embeddings from the metadata.\n",
    "* **print_text_to_image_citation():** Prints the source (citation) and the details of retrieved images from the `get_similar_image_from_query()` fuction.\n",
    "* **get_gemini_response():** Interacts with a Gemini model to answer questions based on a combination of text and image inputs.\n",
    "* **display_images():**  Displays a series of images provided as paths or PIL Image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1707914180634,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "Tngn_vrIKdE1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.intro_multimodal_rag_utils import (\n",
    "    get_similar_text_from_query,\n",
    "    print_text_to_text_citation,\n",
    "    get_similar_image_from_query,\n",
    "    print_text_to_image_citation,\n",
    "    get_gemini_response,\n",
    "    display_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efJPPrzRhvIT"
   },
   "source": [
    "## Multimodal retrieval augmented generation (RAG)\n",
    "\n",
    "Let's bring everything together to implement multimodal RAG. You will use all the elements that you've explored in previous sections to implement the multimodal RAG. These are the steps:\n",
    "\n",
    "* **Step 1:** The user gives a query in text format where the expected information is available in the document and is embedded in images and text.\n",
    "* **Step 2:** Find all text chunks from the pages in the documents using a method similar to the one you explored in `Text Search`.\n",
    "* **Step 3:** Find all similar images from the pages based on the user query matched with `image_description` using a method identical to the one you explored in `Image Search`.\n",
    "* **Step 4:** Combine all similar text and images found in steps 2 and 3 as `context_text` and `context_images`.\n",
    "* **Step 5:** With the help of Gemini, we can pass the user query with text and image context found in steps 2 & 3. You can also add a specific instruction the model should remember while answering the user query.\n",
    "* **Step 6:** Gemini produces the answer, and you can print the citations to check all relevant text and images used to address the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI62Hzuw_0_b"
   },
   "source": [
    "### Step 1: User query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1707900140679,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "XvTKFwOPHLQ_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this time we are not passing any images, but just a simple text query.\n",
    "\n",
    "query = \"\"\"What are the ten best questions to ask for the third quarter of BMW based on the first two conference calls Q&A sections? Write them in bullet points. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUqlkKUaYvZA"
   },
   "source": [
    "### Step 2: Get all relevant text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1707900143343,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "r65yBb5gR_NG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant chunks of text based on the query\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=10,\n",
    "    chunk_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIgXgVIpYzxj"
   },
   "source": [
    "### Step 3: Get all relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707900144005,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "wzu5Gf4yR_J4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all relevant images based on user query\n",
    "matching_results_image_fromdescription_data = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",\n",
    "    image_emb=False,\n",
    "    top_n=10,\n",
    "    embedding_size=1408,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhUpWlGAY2uG"
   },
   "source": [
    "### Step 4: Create context_text and context_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1707900146010,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "B_EEuuLCe6Y5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine all the selected relevant text chunks\n",
    "context_text = []\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.append(value[\"chunk_text\"])\n",
    "final_context_text = \"\\n\".join(context_text)\n",
    "\n",
    "# combine all the relevant images and their description generated by Gemini\n",
    "context_images = []\n",
    "for key, value in matching_results_image_fromdescription_data.items():\n",
    "    context_images.extend(\n",
    "        [\"Image: \", value[\"image_object\"], \"Caption: \", value[\"image_description\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHrtodcBAEu9"
   },
   "source": [
    "### Step 5: Pass context to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "executionInfo": {
     "elapsed": 14374,
     "status": "ok",
     "timestamp": 1707900162344,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "aZuhtJu7fW4n",
    "outputId": "3992482f-b1cf-449d-a961-35d19df807fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\" Instructions: Compare the images and the text provided as Context: to answer multiple Question:\n",
    "Make sure to think thoroughly before answering the question and put the necessary steps to arrive at the answer in bullet points for easy explainability.\n",
    "\n",
    "Context:\n",
    " - Text Context:\n",
    " {final_context_text}\n",
    " - Image Context:\n",
    " {context_images}\n",
    "\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate Gemini response with streaming output\n",
    "response = get_gemini_response(\n",
    "        multimodal_model,\n",
    "        model_input=[prompt],\n",
    "        stream=True,\n",
    "        generation_config=GenerationConfig(temperature=0.4, max_output_tokens=2048),\n",
    "    )\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0FtXYl1fzKh"
   },
   "source": [
    "### Step 6: Print citations and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1454,
     "status": "ok",
     "timestamp": 1707900195260,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "IYRLQ47or1I8",
    "outputId": "92974d77-3616-4001-bcd5-4eb9d57c1a54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"---------------Matched Images------------------\\n\")\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image_fromdescription_data[0][\"img_path\"],\n",
    "        matching_results_image_fromdescription_data[1][\"img_path\"],\n",
    "        matching_results_image_fromdescription_data[2][\"img_path\"],\n",
    "        matching_results_image_fromdescription_data[3][\"img_path\"],\n",
    "        #matching_results_image_fromdescription_data[4][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note: \n",
    "\n",
    "If you run into an error here, it's since the provided documents contain less than 4 images. If you work with your own data you might even be interested in more images. You can adapt this by deleting the \"#\" in the previous code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1707900199706,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "buwd_gp6HJ5K",
    "outputId": "14927077-ea1f-43ab-9d41-99cdb67bba24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image citations. You can check how Gemini generated metadata helped in grounding the answer.\n",
    "\n",
    "print_text_to_image_citation(\n",
    "    matching_results_image_fromdescription_data, print_top=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1707900201676,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "06vYM4MOHJ1-",
    "outputId": "608067bd-360e-405e-be30-cf41056173e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text citations\n",
    "\n",
    "print_text_to_text_citation(\n",
    "    matching_results_chunks_data,\n",
    "    print_top=False,\n",
    "    chunk_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwNrHCqbi3xi"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a RAG system goes beyond simply measuring the final output. It delves into the effectiveness of each stage of the pipeline:\n",
    "\n",
    "- Retrieval Component: Did it identify relevant and accurate information from the external source that addresses the user's query?\n",
    "- Generation Component: Did the LLM effectively utilize the retrieved context to generate a high-quality response?\n",
    "\n",
    "By focusing on these two aspects, we gain valuable insights into the overall performance of the RAG system. This allows us to:\n",
    "\n",
    "- Identify weaknesses: Are irrelevant documents being retrieved? Is the LLM struggling to use the retrieved context effectively?\n",
    "- Optimize performance: Can adjustments be made to the retrieval algorithm or the LLM training process to improve accuracy and coherence?\n",
    "\n",
    "Benchmark against other systems: How does your RAG system compare to other approaches in terms of retrieval effectiveness and generation quality?\n",
    "\n",
    "### Methods for RAG Evaluation\n",
    "\n",
    "Several methods can be employed to evaluate a RAG system, focusing on both context and output:\n",
    "\n",
    "Context-Based Evaluation:   Assesses the quality of retrieved information. This includes metrics like:\n",
    "- Grounding/Faithfulness:   How well the generated response aligns with the retrieved context.\n",
    "- Context Recall:           Whether the retrieved context contains the necessary information for an accurate answer.\n",
    "- Cosine similarity \n",
    "\n",
    "Beyond these general methods, domain-specific metrics can be designed to cater to specific applications. For example, in a question-answering system, the average number of retrieved documents needed for the LLM to generate a correct answer might be a valuable metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Grounding/Faithfulness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.intro_multimodal_rag_utils import compare_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text1 = response\n",
    "text2 = matching_results_chunks_data[0][\"chunk_text\"]\n",
    "\n",
    "results = compare_text(text1, text2)\n",
    "\n",
    "print(f\"Character Level Accuracy: {results['character_level_accuracy']:.2f}%\")\n",
    "print(f\"Word Level Accuracy: {results['word_level_accuracy']:.2f}%\")\n",
    "print(f\"Cosine Similarity: {results['cosine_similarity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Context Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.intro_multimodal_rag_utils import assess_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example usage (without spaCy)\n",
    "context = str(matching_results_chunks_data.items())\n",
    "question = query\n",
    "\n",
    "assessment_results = assess_context(context, question)\n",
    "print(assessment_results)\n",
    "\n",
    "# Example usage (with spaCy for named entity recognition)\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "context = str(matching_results_chunks_data.items())\n",
    "question = query\n",
    "\n",
    "assessment_results = assess_context(context, question, nlp)\n",
    "print(assessment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity of concrete questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define your two texts. THESE ARE ONLY EXAMPLES\n",
    "text1 = \"How is the company investing in its future and what are the key areas of focus?\"\n",
    "text2 = \"When we consider BMWs portfolio, are you seeing the competition across your portfolio or only on certain vehicles such as compact cars?\"\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the texts to TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "# Compute the cosine similarity\n",
    "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "print(f\"Cosine similarity between the two texts is: {similarity[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05jynhZnkgxn"
   },
   "source": [
    "Congratulations on making it through this multimodal RAG notebook!\n",
    "\n",
    "While multimodal RAG can be quite powerful, note that it can face some limitations:\n",
    "\n",
    "* **Data dependency:** Needs high-quality paired text and visuals.\n",
    "* **Computationally demanding:** Processing multimodal data is resource-intensive.\n",
    "* **Domain specific:** Models trained on general data may not shine in specialized fields like medicine.\n",
    "* **Black box:** Understanding how these models work can be tricky, hindering trust and adoption.\n",
    "\n",
    "\n",
    "Despite these challenges, multimodal RAG represents a significant step towards search and retrieval systems that can handle diverse, multimodal data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
